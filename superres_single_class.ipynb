{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from matplotlib import pyplot as plt\n",
    "from superresolution import Superresolution\n",
    "from utils import get_img_paths, load_image, create_mask, plot_prediction\n",
    "from model import DeeplabV3Plus\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "BASE_DIR = os.getcwd()\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "PASCAL_ROOT = os.path.join(DATA_DIR, \"VOCdevkit\", \"VOC2012\")\n",
    "IMGS_PATH = os.path.join(PASCAL_ROOT, \"JPEGImages\")\n",
    "\n",
    "precomputed_dest_root = os.path.join(DATA_DIR, \"precomputed_features\")\n",
    "if not os.path.exists(precomputed_dest_root):\n",
    "    os.mkdir(precomputed_dest_root)\n",
    "\n",
    "SEED = np.random.randint(0, 1000)\n",
    "IMG_SIZE = (512, 512)\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 1000\n",
    "EPOCHS = 30\n",
    "CLASSES = 21\n",
    "RESHAPE_MASKS = True\n",
    "NUM_AUG = 70\n",
    "CLASS_ID = 8 # Cat class"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "def filter_by_class(img_paths, class_id, image_size=(512, 512)):\n",
    "    \"\"\"\n",
    "    Given a list of image paths, return the images that contain the given class id in the respective mask\n",
    "\n",
    "    Args:\n",
    "        img_paths: List of image paths to check\n",
    "        class_id: Class id used for filering\n",
    "        image_size: Size of the image used to load and resize the image\n",
    "\n",
    "    Returns: A dictionary whose keys are the image filename and values are the actual images\n",
    "\n",
    "    \"\"\"\n",
    "    images_dict = {}\n",
    "    for img_path in img_paths:\n",
    "        image_name = os.path.splitext(os.path.basename(img_path))[0]\n",
    "        mask_path = img_path.replace(\"JPEGImages\", \"SegmentationClassAug\").replace(\"jpg\", \"png\")\n",
    "        mask = load_image(mask_path, image_size=image_size, normalize=False, is_png=True, resize_method=\"nearest\")\n",
    "        if np.any(mask == class_id):\n",
    "            image = load_image(img_path, image_size=IMG_SIZE, normalize=True)\n",
    "            images_dict[image_name] = image\n",
    "\n",
    "    print(f\"Valid images: {len(images_dict)} (Initial:  {len(img_paths)})\")\n",
    "    return images_dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid images: 47 (Initial:  500)\n"
     ]
    }
   ],
   "source": [
    "image_list_path = os.path.join(DATA_DIR, \"augmented_file_lists\", \"valaug.txt\")\n",
    "image_paths = get_img_paths(image_list_path, IMGS_PATH)[:500]\n",
    "images_dict = filter_by_class(image_paths, class_id=8)\n",
    "\n",
    "valid_filenames = list(images_dict.keys())\n",
    "\n",
    "model_no_upsample = DeeplabV3Plus(\n",
    "    input_shape=(512, 512, 3),\n",
    "    classes=21,\n",
    "    OS=16,\n",
    "    last_activation=None,\n",
    "    load_weights=True,\n",
    "    backbone=\"mobilenet\",\n",
    "    alpha=1.).build_model(final_upsample=False)\n",
    "\n",
    "model_standard = DeeplabV3Plus(\n",
    "    input_shape=(512, 512, 3),\n",
    "    classes=21,\n",
    "    OS=16,\n",
    "    last_activation=None,\n",
    "    load_weights=True,\n",
    "    backbone=\"mobilenet\",\n",
    "    alpha=1.).build_model(final_upsample=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Save standard output for comparison"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "def get_prediction(model, input_image):\n",
    "\n",
    "    prediction = model.predict(input_image[tf.newaxis, ...])\n",
    "    mask = create_mask(prediction[0])\n",
    "\n",
    "    return mask\n",
    "\n",
    "def save_standard_output(image_dict, model, standard_out_folder, filter_class_id=None):\n",
    "    standard_masks = {}\n",
    "    if not os.path.exists(standard_out_folder):\n",
    "        os.mkdir(standard_out_folder)\n",
    "\n",
    "    for key in tqdm(image_dict):\n",
    "        standard_mask = get_prediction(model, image_dict[key])\n",
    "        if filter_class_id is not None:\n",
    "            standard_mask = tf.where(standard_mask == filter_class_id, standard_mask, 0) # Set to 0 all predictions different from the given class\n",
    "        tf.keras.utils.save_img(f\"{standard_out_folder}/{key}.png\", standard_mask, scale=False)\n",
    "        standard_masks[key] = standard_mask\n",
    "    return standard_masks"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:08<00:00,  5.70it/s]\n"
     ]
    }
   ],
   "source": [
    "standard_out_folder = os.path.join(DATA_DIR, \"standard_output\")\n",
    "standard_masks_dict = save_standard_output(images_dict, model_standard, standard_out_folder, filter_class_id=CLASS_ID)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Precompute Augmented Output Features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "def augment_images(batched_images, angles, shifts):\n",
    "\n",
    "    rotated_images = tfa.image.rotate(batched_images, angles, interpolation=\"bilinear\")\n",
    "    translated_images = tfa.image.translate(rotated_images, shifts, interpolation=\"bilinear\")\n",
    "\n",
    "    return translated_images\n",
    "\n",
    "\n",
    "def save_augmented_features(model, images_array, dest_folder, filter_class_id=None):\n",
    "    if not os.path.exists(dest_folder):\n",
    "        os.mkdir(dest_folder)\n",
    "\n",
    "    predictions = model.predict(images_array, batch_size=2)\n",
    "\n",
    "    for i, prediction in enumerate(predictions):\n",
    "        mask = create_mask(prediction)\n",
    "        if filter_class_id is not None:\n",
    "            mask = tf.where(mask == filter_class_id, mask, 0) # Set to 0 all predictions different from the given class\n",
    "\n",
    "\n",
    "        mask_npy = mask.numpy()\n",
    "        mask_scaled = ((mask_npy - mask_npy.min()) * (1/(mask_npy.max() - mask_npy.min()) * 255)).astype('uint8')\n",
    "\n",
    "        tf.keras.utils.save_img(f\"{dest_folder}/{i}.png\", mask_scaled, scale=False)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def precompute_augmented_features(image_filenames, dest_root_folder, model, class_id=None, num_aug=100, angle_max=0.5, shift_max=30):\n",
    "    for filename in tqdm(image_filenames):\n",
    "        image_path = os.path.join(IMGS_PATH, f\"{filename}.jpg\")\n",
    "        image = load_image(image_path, image_size=(512, 512), normalize=True)\n",
    "        batched_image = tf.tile(tf.expand_dims(image, axis=0), [num_aug, 1, 1, 1])  # Size [num_aug, 512, 512, 3]\n",
    "        angles = np.random.uniform(-angle_max, angle_max, num_aug)\n",
    "        shifts = np.random.uniform(-shift_max, shift_max, (num_aug, 2))\n",
    "        # First sample is not augmented\n",
    "        angles[0] = 0\n",
    "        shifts[0] = np.array([0, 0])\n",
    "        angles = angles.astype(\"float32\")\n",
    "        shifts = shifts.astype(\"float32\")\n",
    "\n",
    "        augmented_images = augment_images(batched_image, angles, shifts)\n",
    "\n",
    "        dest_folder = os.path.join(dest_root_folder, filename)\n",
    "\n",
    "        save_augmented_features(model, augmented_images, dest_folder=dest_folder, filter_class_id=class_id)\n",
    "        np.save(os.path.join(dest_folder, f\"{filename}_angles\"), angles)\n",
    "        np.save(os.path.join(dest_folder, f\"{filename}_shifts\"), shifts)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "angle_max = 0.5  # in radians\n",
    "shift_max = 30\n",
    "\n",
    "precompute_augmented_features(valid_filenames, precomputed_dest_root, model_no_upsample, class_id=CLASS_ID, num_aug=NUM_AUG,\n",
    "                              angle_max=angle_max, shift_max=shift_max)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Compute Super-Resolution Output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_images(img_folder):\n",
    "    images = []\n",
    "    # Sort images based on their filename which is an integer indicating the augmented copy number\n",
    "    image_list = sorted([name.replace(\".png\", \"\") for name in os.listdir(img_folder) if \".npy\" not in name], key=int)\n",
    "\n",
    "    for img_name in image_list:\n",
    "        if \".npy\" in img_name:\n",
    "            continue\n",
    "        image = load_image(os.path.join(img_folder, f\"{img_name}.png\"), normalize=False, is_png=True)\n",
    "        images.append(image)\n",
    "\n",
    "    return images\n",
    "\n",
    "\n",
    "def get_precomputed_folders_path(root_dir, num_aug=100):\n",
    "    valid_folders = []\n",
    "    for path in os.listdir(root_dir):\n",
    "        full_path = os.path.join(root_dir, path)\n",
    "        if len(os.listdir(full_path)) == (num_aug + 2):\n",
    "            valid_folders.append(full_path)\n",
    "        else:\n",
    "            print(f\"Skipped folder named {path} as it is not valid\")\n",
    "\n",
    "    return valid_folders\n",
    "\n",
    "\n",
    "def compute_save_final_output(superresolution_obj, image_filenames, precomputed_root_dir, output_folder, num_aug=100):\n",
    "\n",
    "    superres_masks = {}\n",
    "\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.mkdir(output_folder)\n",
    "\n",
    "    for filename in tqdm(image_filenames):\n",
    "        precomputed_folder_path = os.path.join(precomputed_root_dir, filename)\n",
    "\n",
    "        if not len(os.listdir(precomputed_folder_path)) == (num_aug + 2):\n",
    "            print(f\"Skipped folder named {filename} as it is not valid\")\n",
    "            continue\n",
    "\n",
    "        augmented_images = tf.stack(load_images(precomputed_folder_path))\n",
    "        max_values = np.max(augmented_images, axis=(1, 2, 3), keepdims=True)\n",
    "        max_values[max_values == 0.] = 1.\n",
    "        augmented_images = augmented_images / max_values # TODO: Testing only\n",
    "        augmented_images = tf.cast(augmented_images, tf.float32)\n",
    "\n",
    "        base_name = os.path.basename(os.path.normpath(precomputed_folder_path))\n",
    "        angles = np.load(os.path.join(precomputed_folder_path, f\"{base_name}_angles.npy\"))\n",
    "        shifts = np.load(os.path.join(precomputed_folder_path, f\"{base_name}_shifts.npy\"))\n",
    "        target_image = superresolution_obj.compute_output(augmented_images, angles, shifts)\n",
    "\n",
    "        target_image_npy = target_image[0].numpy()\n",
    "        target_image_scaled = ((target_image_npy - target_image_npy.min()) * (1/(target_image_npy.max() - target_image_npy.min()) * 255)).astype('uint8')\n",
    "\n",
    "        #TODO: handle image scaling\n",
    "        tf.keras.utils.save_img(f\"{output_folder}/{base_name}.png\", target_image_scaled, scale=False)\n",
    "\n",
    "        superres_masks[base_name] = target_image[0]\n",
    "\n",
    "    return superres_masks"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# super resolution parameters\n",
    "learning_rate = 1e-3\n",
    "lambda_eng = 0.0001 * NUM_AUG\n",
    "lambda_tv = 0.002 * NUM_AUG\n",
    "num_iter = 400\n",
    "\n",
    "superresolution = Superresolution(\n",
    "    lambda_tv=lambda_tv,\n",
    "    lambda_eng=lambda_eng,\n",
    "    num_iter=num_iter,\n",
    "    num_aug=NUM_AUG,\n",
    "    learning_rate=learning_rate\n",
    ")\n",
    "\n",
    "precomputed_root_dir = os.path.join(DATA_DIR, \"precomputed_features\")\n",
    "output_folder = os.path.join(DATA_DIR, \"superres_output\")\n",
    "\n",
    "superres_masks_dict = compute_save_final_output(superresolution, valid_filenames, precomputed_root_dir, output_folder, num_aug=NUM_AUG)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def Mean_IOU(y_true, y_pred):\n",
    "    nb_classes = 21  # TODO: set this as a parameter\n",
    "    ious = []\n",
    "    for i in range(0, nb_classes):  # exclude last label (void)\n",
    "        y_true_squeeze = tf.squeeze(y_true)\n",
    "        y_pred_squeeze = tf.squeeze(y_pred)\n",
    "        true_labels = tf.equal(y_true_squeeze, i)\n",
    "        pred_labels = tf.equal(y_pred_squeeze, i)\n",
    "        inter = tf.cast(true_labels & pred_labels, tf.int32)\n",
    "        union = tf.cast(true_labels | pred_labels, tf.int32)\n",
    "\n",
    "        iou = tf.reduce_sum(inter) / tf.reduce_sum(union)\n",
    "        # returns average IoU of the same objects\n",
    "        ious.append(iou)\n",
    "\n",
    "    ious = tf.stack(ious)\n",
    "    legal_labels = ~tf.math.is_nan(ious)\n",
    "    ious = tf.gather(ious, indices=tf.where(legal_labels))\n",
    "    return tf.reduce_mean(ious)\n",
    "\n",
    "\n",
    "def custom_IOU(y_true, y_pred, class_id):\n",
    "    y_true_squeeze = tf.squeeze(y_true)\n",
    "    y_pred_squeeze = tf.squeeze(y_pred)\n",
    "    classes = [0, class_id] # Only check in background and given class\n",
    "\n",
    "    y_true_squeeze = tf.where(y_true_squeeze != class_id, 0, y_true_squeeze)\n",
    "\n",
    "    ious = []\n",
    "    for i in classes:\n",
    "        true_labels = tf.equal(y_true_squeeze, i)\n",
    "        pred_labels = tf.equal(y_pred_squeeze, i)\n",
    "        inter = tf.cast(true_labels & pred_labels, tf.int32)\n",
    "        union = tf.cast(true_labels | pred_labels, tf.int32)\n",
    "\n",
    "        iou = tf.reduce_sum(inter) / tf.reduce_sum(union)\n",
    "        ious.append(iou)\n",
    "\n",
    "    ious = tf.stack(ious)\n",
    "    legal_labels = ~tf.math.is_nan(ious)\n",
    "    ious = tf.gather(ious, indices=tf.where(legal_labels))\n",
    "    return tf.reduce_mean(ious)\n",
    "\n",
    "\n",
    "def evaluate_IOU(true_mask, standard_mask, superres_mask, img_size=(512, 512)):\n",
    "    true_mask = tf.reshape(true_mask, (img_size[0] * img_size[1], 1))\n",
    "    standard_mask = tf.reshape(standard_mask, (img_size[0] * img_size[1], 1))\n",
    "    superres_mask = tf.reshape(superres_mask, (img_size[0] * img_size[1], 1))\n",
    "\n",
    "    # standard_IOU = Mean_IOU(true_mask, standard_mask)\n",
    "    # superres_IOU = Mean_IOU(true_mask, superres_mask)\n",
    "\n",
    "    standard_IOU = custom_IOU(true_mask, standard_mask, class_id=CLASS_ID)\n",
    "    superres_IOU = custom_IOU(true_mask, superres_mask, class_id=CLASS_ID)\n",
    "\n",
    "    return standard_IOU.numpy(), superres_IOU.numpy()\n",
    "\n",
    "def compare_results(image_dict, standard_dict, superres_dict, image_size=(512, 512)):\n",
    "    standard_IOUs = []\n",
    "    superres_IOUs = []\n",
    "\n",
    "    for key in image_dict:\n",
    "        true_mask_path = os.path.join(DATA_DIR, \"VOCdevkit/VOC2012/SegmentationClassAug\", f\"{key}.png\")\n",
    "        true_mask = load_image(true_mask_path, image_size=image_size, normalize=False,\n",
    "                               is_png=True, resize_method=\"nearest\")\n",
    "\n",
    "        standard_mask = standard_dict[key]\n",
    "        superres_image = superres_dict[key]\n",
    "\n",
    "        standard_IOU, superres_IOU = evaluate_IOU(true_mask, standard_mask, superres_image, img_size=image_size)\n",
    "        standard_IOUs.append(standard_IOU)\n",
    "        superres_IOUs.append(superres_IOU)\n",
    "        print(f\"IOUs for image {key} - Standard: {str(standard_IOU)}, Superres: {str(superres_IOU)}\")\n",
    "\n",
    "    return standard_IOUs, superres_IOUs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "superres_masks_dict_th = {}\n",
    "\n",
    "for key in superres_masks_dict:\n",
    "    sample_th = tf.cast(tf.reduce_max(superres_masks_dict[key]), tf.float32) * 0.15\n",
    "    th_mask = tf.where(superres_masks_dict[key] > sample_th, CLASS_ID, 0)\n",
    "    superres_masks_dict_th[key] = th_mask\n",
    "\n",
    "standard_IOUs, superres_IOUs = compare_results(images_dict, standard_masks_dict, superres_masks_dict_th, image_size=IMG_SIZE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.mean(standard_IOUs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.mean(superres_IOUs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tests"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_standard_superres(input_image, standard_mask, superres_mask):\n",
    "    plt.figure(figsize=(18, 18))\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.title(\"Input Image\")\n",
    "    plt.imshow(tf.keras.preprocessing.image.array_to_img(input_image))\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.title(\"Sandard predicted Mask\")\n",
    "    plt.imshow(tf.keras.preprocessing.image.array_to_img(input_image))\n",
    "    plt.imshow(tf.keras.preprocessing.image.array_to_img(standard_mask), alpha=0.5)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.title(\"Superresolution Mask\")\n",
    "    plt.imshow(tf.keras.preprocessing.image.array_to_img(input_image))\n",
    "    plt.imshow(tf.keras.preprocessing.image.array_to_img(superres_mask), alpha=0.5)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_histogram(image):\n",
    "    plt.figure(figsize=(18, 18))\n",
    "    vals = image.flatten()\n",
    "    b, bins, patches = plt.hist(vals, 255)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def print_labels(masks):\n",
    "    title = [\"Standard Labels: \", \"Superres Labels: \"]\n",
    "    for i in range(2):\n",
    "        values, count = np.unique(masks[i], return_counts=True)\n",
    "        print(title[i] + str(dict(zip(values, count))))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sample_key = random.choice(valid_filenames)\n",
    "sample_image = images_dict[sample_key]\n",
    "sample_standard = standard_masks_dict[sample_key]\n",
    "sample_superres = superres_masks_dict_th[sample_key]\n",
    "\n",
    "true_mask_path = os.path.join(DATA_DIR, \"VOCdevkit/VOC2012/SegmentationClassAug\", f\"{sample_key}.png\")\n",
    "true_mask = load_image(true_mask_path, image_size=IMG_SIZE, normalize=False, is_png=True, resize_method=\"nearest\")\n",
    "\n",
    "plot_prediction([sample_image, true_mask, sample_standard], only_prediction=False, show_overlay=True)\n",
    "print_labels([true_mask, sample_standard])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_standard_superres(sample_image, sample_standard, sample_superres)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "superres_numpy = sample_superres\n",
    "plot_histogram(superres_numpy)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sample_th = np.max(superres_numpy) * 0.15\n",
    "th_mask = tf.where(sample_superres > sample_th, sample_superres, 0).numpy()\n",
    "th_mask_class = tf.where(sample_superres > sample_th, CLASS_ID, 0).numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_histogram(th_mask)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_standard_superres(sample_image, sample_standard, sample_superres)\n",
    "plot_standard_superres(sample_image, sample_standard, th_mask_class)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "evaluate_IOU(true_mask, sample_standard, sample_superres)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.unique(true_mask)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}