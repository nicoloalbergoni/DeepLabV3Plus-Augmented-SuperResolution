{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import random\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from model import DeeplabV3Plus\n",
    "import tensorflow_addons as tfa\n",
    "from matplotlib import pyplot as plt\n",
    "from superresolution_scripts.optimizer import Optimizer\n",
    "from superresolution_scripts.superresolution import Superresolution\n",
    "from utils import *\n",
    "from superresolution_scripts.superres_utils import *\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (512, 512)\n",
    "FEATURE_SIZE = (128, 128)\n",
    "NUM_AUG = 100\n",
    "CLASS_ID = 8\n",
    "NUM_SAMPLES = 500\n",
    "MODE = \"argmax\"\n",
    "MODEL_BACKBONE = \"xception\"\n",
    "USE_VALIDATION = False\n",
    "SAVE_SLICE_OUTPUT = False\n",
    "SAVE_FINAL_SR_OUTPUT = True\n",
    "TH_FACTOR = 0.2\n",
    "ANGLE_MAX = 0.15\n",
    "SHIFT_MAX = 80\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(os.getcwd(), \"data\")\n",
    "PASCAL_ROOT = os.path.join(DATA_DIR, \"dataset_root\", \"VOCdevkit\", \"VOC2012\")\n",
    "IMGS_PATH = os.path.join(PASCAL_ROOT, \"JPEGImages\")\n",
    "\n",
    "TEST_FOLDER_ROOT = os.path.join(os.getcwd(), \"thesis_images_folder\")\n",
    "AUGMENTED_IMAGE_FOLDER = os.path.join(TEST_FOLDER_ROOT, f\"augmented_images_{MODE}_{CLASS_ID}\")\n",
    "\n",
    "STANDARD_OUTPUT_ROOT = os.path.join(TEST_FOLDER_ROOT, \"standard_output\", \"standard_output\")\n",
    "STANDARD_OUTPUT_DIR = os.path.join(\n",
    "    STANDARD_OUTPUT_ROOT, f\"{MODEL_BACKBONE}_{CLASS_ID}{'_validation' if USE_VALIDATION else ''}\")\n",
    "\n",
    "\n",
    "SUPERRES_ROOT = os.path.join(TEST_FOLDER_ROOT, \"superres_root\")\n",
    "SUPERRES_OUTPUT_DIR = os.path.join(\n",
    "    SUPERRES_ROOT, f\"superres_output{'_validation' if USE_VALIDATION else ''}\")\n",
    "\n",
    "if not os.path.exists(AUGMENTED_IMAGE_FOLDER):\n",
    "    os.makedirs(AUGMENTED_IMAGE_FOLDER)\n",
    "if not os.path.exists(SUPERRES_OUTPUT_DIR):\n",
    "    os.makedirs(SUPERRES_OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"2007_000528\"\n",
    "image_path = os.path.join(IMGS_PATH, f\"{filename}.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_augmented_copies(image, num_aug, angle_max, shift_max):\n",
    "    batched_images = tf.tile(tf.expand_dims(image, axis=0), [\n",
    "                             num_aug, 1, 1, 1])  # Size [num_aug, 512, 512, 3]\n",
    "    angles = np.random.uniform(-angle_max, angle_max, num_aug)\n",
    "    shifts = np.random.uniform(-shift_max, shift_max, (num_aug, 2))\n",
    "    # First sample is not augmented\n",
    "    angles[0] = 0\n",
    "    shifts[0] = np.array([0, 0])\n",
    "    angles = angles.astype(\"float32\")\n",
    "    shifts = shifts.astype(\"float32\")\n",
    "\n",
    "    rotated_images = tfa.image.rotate(\n",
    "        batched_images, angles, interpolation=\"bilinear\")\n",
    "    translated_images = tfa.image.translate(\n",
    "        rotated_images, shifts, interpolation=\"bilinear\")\n",
    "\n",
    "    return translated_images, angles, shifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_augmented_features(images_paths, model, dest_folder, filter_class_id, mode=\"slice\", num_aug=100,\n",
    "                               angle_max=0.15, shift_max=80, save_output=False, image_size=(512, 512)):\n",
    "\n",
    "    for image_path in tqdm(images_paths):\n",
    "        image_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "\n",
    "        # Load image\n",
    "        image = load_image(image_path, image_size=image_size, normalize=True)\n",
    "        # Create augmented copies\n",
    "        augmented_copies, angles, shifts = create_augmented_copies(image, num_aug=num_aug, angle_max=angle_max,\n",
    "                                                                   shift_max=shift_max)\n",
    "\n",
    "        # Create destination folder\n",
    "        output_folder = os.path.join(dest_folder, mode, image_name)\n",
    "        if not os.path.exists(output_folder):\n",
    "            os.makedirs(output_folder)\n",
    "\n",
    "        class_masks = []\n",
    "        max_masks = []\n",
    "\n",
    "        predictions = model.predict(augmented_copies, batch_size=BATCH_SIZE)\n",
    "        # Used to clear memory as it appears that there is a memory leak with something related to model.predict\n",
    "        _ = gc.collect()\n",
    "\n",
    "        for i, prediction in enumerate(predictions):\n",
    "            if mode == \"slice\":\n",
    "                # Get the slice corresponding to the class id\n",
    "                class_mask = tf.gather(\n",
    "                    prediction, filter_class_id, axis=-1)[..., tf.newaxis]\n",
    "\n",
    "                # Get all the other slices and compute the max pixel-wise\n",
    "                gather_indexes = np.delete(\n",
    "                    np.arange(0, tf.shape(prediction)[-1], step=1), filter_class_id)\n",
    "                max_mask = tf.reduce_max(\n",
    "                    tf.gather(prediction, gather_indexes, axis=-1), axis=-1)[..., tf.newaxis]\n",
    "\n",
    "                max_masks.append(max_mask)\n",
    "\n",
    "            elif mode == \"slice_var\":\n",
    "                # Get the slice corresponding to the class id\n",
    "                class_mask = tf.gather(\n",
    "                    prediction, filter_class_id, axis=-1)[..., tf.newaxis]\n",
    "\n",
    "                global_max = tf.reduce_max(prediction)\n",
    "                global_min = tf.reduce_min(prediction)\n",
    "\n",
    "                class_mask = min_max_normalization(class_mask.numpy(), new_min=0.0, new_max=1.0, global_min=global_min,\n",
    "                                                   global_max=global_max)\n",
    "\n",
    "            else:\n",
    "                class_mask = create_mask(prediction)\n",
    "                # Set to 0 all predictions different from the given class\n",
    "                class_mask = tf.where(\n",
    "                    class_mask == filter_class_id, class_mask, 0)\n",
    "                # Necessary for super-resolution operations\n",
    "                class_mask = tf.cast(class_mask, tf.float32)\n",
    "                class_mask = class_mask.numpy()\n",
    "\n",
    "            class_masks.append(class_mask)\n",
    "\n",
    "            if save_output == 0:\n",
    "                tf.keras.utils.save_img(\n",
    "                    f\"{output_folder}/{i}_class.png\", class_mask, scale=True)\n",
    "                if mode == \"slice\":\n",
    "                    tf.keras.utils.save_img(\n",
    "                        f\"{output_folder}/{i}_max.png\", max_mask, scale=True)\n",
    "    \n",
    "    return class_masks, max_masks, angles, shifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeeplabV3Plus(\n",
    "    input_shape=(512, 512, 3),\n",
    "    classes=21,\n",
    "    OS=16,\n",
    "    last_activation=None,\n",
    "    load_weights=True,\n",
    "    backbone=MODEL_BACKBONE,\n",
    "    alpha=1.).build_model(final_upsample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_masks, max_masks, angles, shifts = compute_augmented_features([image_path], model, mode=MODE, dest_folder=AUGMENTED_IMAGE_FOLDER, filter_class_id=CLASS_ID,\n",
    "                            num_aug=NUM_AUG, angle_max=ANGLE_MAX, shift_max=SHIFT_MAX, save_output=True, image_size=IMG_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_dict = {\n",
    "    \"lambda_df\": 1.0,\n",
    "    \"lambda_tv\": 0.3,\n",
    "    \"lambda_L2\": 0.7,\n",
    "    \"lambda_L1\": 0,\n",
    "}\n",
    "\n",
    "optimizer_obj = Optimizer(optimizer=\"adam\", learning_rate=1e-3, amsgrad=True, lr_scheduler=True, decay_steps=60, decay_rate=30)\n",
    "\n",
    "superresolution_obj = Superresolution(**coeff_dict, num_iter=300, num_aug=100, optimizer=optimizer_obj, feature_size=FEATURE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true_mask_path = os.path.join(\n",
    "#     PASCAL_ROOT, \"SegmentationClassAug\", f\"{filename}.png\")\n",
    "# true_mask = load_image(true_mask_path, image_size=IMG_SIZE, normalize=False,\n",
    "#                         is_png=True, resize_method=\"nearest\")\n",
    "\n",
    "# standard_mask_path = os.path.join(\n",
    "#     STANDARD_OUTPUT_DIR, f\"{filename}.png\")\n",
    "# standard_mask = load_image(standard_mask_path, image_size=IMG_SIZE, normalize=False, is_png=True,\n",
    "#                             resize_method=\"nearest\")\n",
    "\n",
    "max_masks = None\n",
    "\n",
    "target_augmented_SR = compute_SR(superresolution_obj, class_masks, angles, shifts, filename, max_masks=max_masks, SR_type=\"aug\", save_final_output=SAVE_FINAL_SR_OUTPUT,\n",
    "                                    save_intermediate_output=SAVE_SLICE_OUTPUT, class_id=CLASS_ID, dest_folder=SUPERRES_OUTPUT_DIR, th_factor=TH_FACTOR)\n",
    "\n",
    "target_max_SR = compute_SR(superresolution_obj, class_masks, angles, shifts, filename, max_masks=max_masks, SR_type=\"max\", save_final_output=SAVE_FINAL_SR_OUTPUT,\n",
    "                            save_intermediate_output=SAVE_SLICE_OUTPUT, class_id=CLASS_ID, dest_folder=SUPERRES_OUTPUT_DIR, th_factor=TH_FACTOR)\n",
    "\n",
    "target_mean_SR = compute_SR(superresolution_obj, class_masks, angles, shifts, filename, max_masks=max_masks, SR_type=\"mean\", save_final_output=SAVE_FINAL_SR_OUTPUT,\n",
    "                            save_intermediate_output=SAVE_SLICE_OUTPUT, class_id=CLASS_ID, dest_folder=SUPERRES_OUTPUT_DIR, th_factor=TH_FACTOR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('venv38': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "475ae36d3f1652f0752efb619578b7a4687dd826eecdfd447188c8fb7138b929"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
