{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# from kaggle_secrets import UserSecretsClient\n",
    "# user_secrets = UserSecretsClient()\n",
    "# TOKEN = user_secrets.get_secret(\"GITHUB_TOKEN\")\n",
    "#\n",
    "# !git clone -b parameters_search https://{TOKEN}@github.com/nicoloalbergoni/DeeplabV3Plus-TF2.git\n",
    "# %load_ext tensorboard\n",
    "# %cd ./DeeplabV3Plus-TF2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "#!python download_and_prepare_voc.py --remove_cmap --download_berkley"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import wandb\n",
    "import h5py\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from model import DeeplabV3Plus\n",
    "import tensorflow_addons as tfa\n",
    "from matplotlib import pyplot as plt\n",
    "from superresolution_scripts.superresolution import Superresolution\n",
    "from utils import load_image, get_prediction, create_mask, plot_prediction\n",
    "from superresolution_scripts.superres_utils import get_img_paths, filter_by_class, min_max_normalization, plot_image, plot_histogram, list_precomputed_data_paths, check_hdf5_validity, threshold_image, single_class_IOU\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "BASE_DIR = os.getcwd()\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "PASCAL_ROOT = os.path.join(DATA_DIR, \"dataset_root\", \"VOCdevkit\", \"VOC2012\")\n",
    "IMGS_PATH = os.path.join(PASCAL_ROOT, \"JPEGImages\")\n",
    "\n",
    "SUPERRES_ROOT = os.path.join(DATA_DIR, \"superres_root\")\n",
    "PRECOMPUTED_OUTPUT_DIR = os.path.join(SUPERRES_ROOT, \"precomputed_features\")\n",
    "STANDARD_OUTPUT_DIR = os.path.join(SUPERRES_ROOT, \"standard_output\")\n",
    "SUPERRES_OUTPUT_DIR = os.path.join(SUPERRES_ROOT, \"superres_output\")\n",
    "\n",
    "# SEED = np.random.randint(0, 1000)\n",
    "SEED = 1234\n",
    "IMG_SIZE = (512, 512)\n",
    "BATCH_SIZE = 2\n",
    "BUFFER_SIZE = 1000\n",
    "EPOCHS = 30\n",
    "CLASSES = 21\n",
    "RESHAPE_MASKS = True\n",
    "NUM_AUG = 100\n",
    "CLASS_ID = 8\n",
    "NUM_SAMPLES = 300\n",
    "MODE = \"slice\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid images: 28 (Initial:  300)\n"
     ]
    }
   ],
   "source": [
    "image_list_path = os.path.join(DATA_DIR, \"augmented_file_lists\", \"trainaug.txt\")\n",
    "image_paths = get_img_paths(image_list_path, IMGS_PATH)\n",
    "\n",
    "if NUM_SAMPLES is not None:\n",
    "    image_paths = image_paths[:NUM_SAMPLES]\n",
    "\n",
    "images_dict = filter_by_class(image_paths, class_id=CLASS_ID)\n",
    "\n",
    "print(f\"Valid images: {len(images_dict)} (Initial:  {len(image_paths)})\")\n",
    "\n",
    "valid_filenames = list(images_dict.keys())\n",
    "\n",
    "model_no_upsample = DeeplabV3Plus(\n",
    "    input_shape=(512, 512, 3),\n",
    "    classes=21,\n",
    "    OS=16,\n",
    "    last_activation=None,\n",
    "    load_weights=True,\n",
    "    backbone=\"mobilenet\",\n",
    "    alpha=1.).build_model(final_upsample=False)\n",
    "\n",
    "model_standard = DeeplabV3Plus(\n",
    "    input_shape=(512, 512, 3),\n",
    "    classes=21,\n",
    "    OS=16,\n",
    "    last_activation=None,\n",
    "    load_weights=True,\n",
    "    backbone=\"mobilenet\",\n",
    "    alpha=1.).build_model(final_upsample=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Compute standard output for comparison"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def compute_standard_output(image_dict, model, dest_folder, filter_class_id=None):\n",
    "    standard_masks = {}\n",
    "    if not os.path.exists(dest_folder):\n",
    "        os.makedirs(dest_folder)\n",
    "\n",
    "    for key in tqdm(image_dict):\n",
    "        standard_mask = get_prediction(model, image_dict[key])\n",
    "        if filter_class_id is not None:\n",
    "            standard_mask = tf.where(standard_mask == filter_class_id, standard_mask, 0) # Set to 0 all predictions different from the given class\n",
    "        tf.keras.utils.save_img(f\"{dest_folder}/{key}.png\", standard_mask, scale=False)\n",
    "        standard_masks[key] = standard_mask\n",
    "\n",
    "    return standard_masks"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:17<00:00,  1.58it/s]\n"
     ]
    }
   ],
   "source": [
    "standard_masks_dict = compute_standard_output(images_dict, model_standard, dest_folder=STANDARD_OUTPUT_DIR, filter_class_id=CLASS_ID)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Precompute Augmented Output Features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def create_augmented_copies(image, num_aug, angle_max, shift_max, chunk_size=50):\n",
    "\n",
    "    if (num_aug % chunk_size) != 0:\n",
    "        raise Exception(\"Num aug must be a multiple of 50\")\n",
    "\n",
    "    num_chunks = num_aug // chunk_size\n",
    "\n",
    "    angles = np.random.uniform(-angle_max, angle_max, num_aug)\n",
    "    shifts = np.random.uniform(-shift_max, shift_max, (num_aug, 2))\n",
    "    angles[0] = 0\n",
    "    shifts[0] = np.array([0, 0])\n",
    "    angles = angles.astype(\"float32\")\n",
    "    shifts = shifts.astype(\"float32\")\n",
    "\n",
    "    angles_chunks = np.split(angles, num_chunks)\n",
    "    shifts_chunks = np.split(shifts, num_chunks)\n",
    "\n",
    "    augmented_chunks = []\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        images_chunk = tf.tile(tf.expand_dims(image, axis=0), [chunk_size, 1, 1, 1])\n",
    "        rotated_chunk = tfa.image.rotate(images_chunk, angles_chunks[i], interpolation=\"bilinear\")\n",
    "        translated_chunk= tfa.image.translate(rotated_chunk, shifts_chunks[i], interpolation=\"bilinear\")\n",
    "        augmented_chunks.append(translated_chunk.numpy())\n",
    "\n",
    "    augmented_copies = np.concatenate(augmented_chunks, axis=0)\n",
    "\n",
    "    return augmented_copies, angles, shifts\n",
    "\n",
    "\n",
    "def compute_augmented_features(image_filenames, model, dest_folder, filter_class_id, mode=\"slice\", num_aug=100, angle_max=0.5, shift_max=30, save_output=False, relu_output=False, chunk_size=50):\n",
    "\n",
    "    augmented_features = {}\n",
    "\n",
    "    for filename in tqdm(image_filenames):\n",
    "\n",
    "        # Load image\n",
    "        image_path = os.path.join(IMGS_PATH, f\"{filename}.jpg\")\n",
    "        image = load_image(image_path, image_size=IMG_SIZE, normalize=True)\n",
    "\n",
    "        # Create augmented copies\n",
    "        augmented_copies, angles, shifts = create_augmented_copies(image, num_aug=num_aug, angle_max=angle_max, shift_max=shift_max, chunk_size=chunk_size)\n",
    "\n",
    "        # Create destination folder\n",
    "        output_folder = os.path.join(dest_folder, filename)\n",
    "        if not os.path.exists(output_folder):\n",
    "            os.makedirs(output_folder)\n",
    "\n",
    "        class_masks = []\n",
    "        max_masks = []\n",
    "\n",
    "        predictions = model.predict(augmented_copies, batch_size=BATCH_SIZE)\n",
    "\n",
    "        for i, prediction in enumerate(predictions):\n",
    "\n",
    "            if mode == \"slice\":\n",
    "                class_slice = prediction[:, :, filter_class_id]\n",
    "                class_mask = class_slice[..., np.newaxis]\n",
    "\n",
    "                no_class_prediction = np.delete(prediction, filter_class_id, axis=-1)\n",
    "                max_mask = no_class_prediction.max(axis=-1)\n",
    "                max_mask = max_mask[..., np.newaxis]\n",
    "\n",
    "                # ReLU is only needed when working with slices\n",
    "                if relu_output:\n",
    "                    class_mask = (tf.nn.relu(class_mask)).numpy()\n",
    "                    max_mask = (tf.nn.relu(max_mask)).numpy()\n",
    "\n",
    "                max_masks.append(max_mask)\n",
    "\n",
    "            elif mode == \"argmax\":\n",
    "                class_mask = create_mask(prediction)\n",
    "                # Set to 0 all predictions different from the given class\n",
    "                class_mask = tf.where(class_mask == filter_class_id, class_mask, 0)\n",
    "                class_mask = tf.cast(class_mask, tf.float32) # Necessary for super-resolution operations\n",
    "                class_mask = class_mask.numpy()\n",
    "\n",
    "            class_masks.append(class_mask)\n",
    "\n",
    "            if save_output:\n",
    "                tf.keras.utils.save_img(f\"{output_folder}/{i}_class.png\", class_mask, scale=True)\n",
    "                if mode == \"slice\":\n",
    "                    tf.keras.utils.save_img(f\"{output_folder}/{i}_max.png\", max_mask, scale=True)\n",
    "\n",
    "            # np.save(os.path.join(output_folder, f\"{filename}_angles\"), angles)\n",
    "            # np.save(os.path.join(output_folder, f\"{filename}_shifts\"), shifts)\n",
    "\n",
    "        file = h5py.File(f\"{output_folder}/{filename}.hdf5\", \"w\")\n",
    "        file.create_dataset(\"class_masks\", data=class_masks)\n",
    "\n",
    "        if mode == \"slice\":\n",
    "            file.create_dataset(\"max_masks\", data=max_masks)\n",
    "\n",
    "        file.create_dataset(\"angles\", data=angles)\n",
    "        file.create_dataset(\"shifts\", data=shifts)\n",
    "        file.attrs[\"filename\"] = filename\n",
    "        file.attrs[\"mode\"] = mode\n",
    "\n",
    "        file.close()\n",
    "\n",
    "        augmented_features[filename] = { \"class\": class_masks, \"max\": max_masks }\n",
    "\n",
    "    return augmented_features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [01:52<00:00,  4.03s/it]\n"
     ]
    }
   ],
   "source": [
    "angle_max = 0.5  # in radians\n",
    "shift_max = 30\n",
    "\n",
    "augmented_features_dict = compute_augmented_features(images_dict, model_no_upsample, mode=MODE, dest_folder=PRECOMPUTED_OUTPUT_DIR, filter_class_id=CLASS_ID, num_aug=NUM_AUG, angle_max=angle_max, shift_max=shift_max, save_output=True, relu_output=False, chunk_size=50)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Compute Super-Resolution Output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def evaluate_IOU(true_mask, superres_mask, img_size=(512, 512)):\n",
    "    true_mask = tf.reshape(true_mask, (img_size[0] * img_size[1], 1))\n",
    "    superres_mask = tf.reshape(superres_mask, (img_size[0] * img_size[1], 1))\n",
    "\n",
    "    superres_IOU = single_class_IOU(true_mask, superres_mask, class_id=CLASS_ID)\n",
    "\n",
    "    return superres_IOU.numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def compute_superresolution_output(precomputed_data_paths, superres_args, dest_folder, mode=\"slice\", num_aug=100, global_normalize=True, save_output=False):\n",
    "\n",
    "    superres_masks = {}\n",
    "    class_losses = {}\n",
    "    ious = {}\n",
    "\n",
    "    if not os.path.exists(dest_folder):\n",
    "        os.makedirs(dest_folder)\n",
    "\n",
    "    for file_path in tqdm(precomputed_data_paths):\n",
    "\n",
    "        file = h5py.File(f\"{file_path}\", \"r\")\n",
    "\n",
    "        if not check_hdf5_validity(file, num_aug=num_aug):\n",
    "            print(f\"File: {file_path} is invalid, skipping...\")\n",
    "            file.close()\n",
    "            continue\n",
    "\n",
    "\n",
    "        filename = file.attrs[\"filename\"]\n",
    "        angles = file[\"angles\"][:]\n",
    "        shifts = file[\"shifts\"][:]\n",
    "\n",
    "        class_masks = file[\"class_masks\"][:]\n",
    "        class_masks = tf.stack(class_masks)\n",
    "\n",
    "        if mode == \"slice\":\n",
    "            max_masks = file[\"max_masks\"][:]\n",
    "            max_masks = tf.stack(max_masks)\n",
    "\n",
    "        file.close()\n",
    "\n",
    "\n",
    "        superresolution_obj = Superresolution(\n",
    "            **superres_args,\n",
    "            num_aug=NUM_AUG,\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "\n",
    "        global_min = tf.reduce_min(class_masks) if global_normalize else None\n",
    "        global_max = tf.reduce_max(class_masks) if global_normalize else None\n",
    "\n",
    "        class_masks = tf.map_fn(fn=lambda image: min_max_normalization(image.numpy(), new_min=0.0, new_max=1.0, global_min=global_min, global_max=global_max), elems=class_masks)\n",
    "\n",
    "        target_image_class, class_loss = superresolution_obj.compute_output(class_masks, angles, shifts)\n",
    "        target_image_class = (target_image_class[0]).numpy()\n",
    "        print(f\"Final class loss for image {filename}: {class_loss}\")\n",
    "\n",
    "        if mode == \"slice\":\n",
    "\n",
    "            global_min = tf.reduce_min(max_masks) if global_normalize else None\n",
    "            global_max = tf.reduce_max(max_masks) if global_normalize else None\n",
    "\n",
    "            max_masks = tf.map_fn(fn=lambda image: min_max_normalization(image.numpy(), new_min=0.0, new_max=1.0, global_min=global_min, global_max=global_max), elems=max_masks)\n",
    "\n",
    "\n",
    "            target_image_max, max_loss = superresolution_obj.compute_output(max_masks, angles, shifts)\n",
    "            target_image_max = (target_image_max[0]).numpy()\n",
    "            print(f\"Final max loss for image {filename}: {max_loss}\")\n",
    "\n",
    "\n",
    "\n",
    "        class_losses[filename] = class_loss\n",
    "\n",
    "        th_image = threshold_image(target_image_class, CLASS_ID, th_mask= None if mode == \"argmax\" else target_image_max, th_factor=.15)\n",
    "\n",
    "        true_mask_path = os.path.join(PASCAL_ROOT, \"SegmentationClassAug\", f\"{filename}.png\")\n",
    "        true_mask = load_image(true_mask_path, image_size=IMG_SIZE, normalize=False,\n",
    "                               is_png=True, resize_method=\"nearest\")\n",
    "\n",
    "        iou = evaluate_IOU(true_mask, th_image)\n",
    "        ious[filename] = iou\n",
    "\n",
    "        if save_output:\n",
    "            tf.keras.utils.save_img(f\"{dest_folder}/{filename}_th_{mode}.png\", th_image, scale=True)\n",
    "\n",
    "\n",
    "    mean_iou = np.mean(np.fromiter(ious.values(), dtype=float))\n",
    "    print(f\"Final Mean IOU: {mean_iou}\")\n",
    "\n",
    "    return mean_iou"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SuperresTuner(kt.RandomSearch):\n",
    "\n",
    "    precomputed_data_paths = list_precomputed_data_paths(PRECOMPUTED_OUTPUT_DIR)\n",
    "\n",
    "    def run_trial(self, trial, **kwargs):\n",
    "        hp = trial.hyperparameters\n",
    "\n",
    "        superres_args = {\n",
    "            \"lambda_tv\": hp.Float(\"lambda_tv\", min_value=0.01, max_value= 2.0),\n",
    "            \"lambda_eng\": hp.Float(\"lambda_eng\", min_value=0.01, max_value= 4.0),\n",
    "            # \"num_iter\": hp.Int(\"num_iter\", min_value=400, max_value=800, step=50),\n",
    "            \"num_iter\": 450,\n",
    "            \"learning_rate\" : 1e-3,\n",
    "            \"loss_coeff\": False\n",
    "        }\n",
    "\n",
    "        global_normalize = True\n",
    "\n",
    "        wandb_dir = os.path.join(DATA_DIR, \"wandb_logs\")\n",
    "        if not os.path.exists(wandb_dir):\n",
    "            os.makedirs(wandb_dir)\n",
    "\n",
    "        run = wandb.init(project=\"tesi\", entity=\"albergoni-nicolo\", dir=wandb_dir ,config=hp.values)\n",
    "\n",
    "        wandb.config.num_aug = NUM_AUG\n",
    "        wandb.config.num_sample = NUM_SAMPLES\n",
    "        wandb.config.batch_size = BATCH_SIZE\n",
    "        wandb.config.class_id = CLASS_ID\n",
    "        wandb.config.num_iter = superres_args[\"num_iter\"]\n",
    "        wandb.config.lr = superres_args[\"learning_rate\"]\n",
    "\n",
    "        metric = compute_superresolution_output(self.precomputed_data_paths, superres_args, dest_folder=SUPERRES_OUTPUT_DIR, mode=MODE, num_aug=NUM_AUG, global_normalize=global_normalize, save_output=False)\n",
    "\n",
    "        run.log({\"mean_iou\": metric})\n",
    "        run.finish()\n",
    "\n",
    "        return 1.0 - metric"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search: Running Trial #1\n",
      "\n",
      "default configuration\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Finishing last run (ID:30p0fn2d) before initializing another..."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "24cb25a9c88e4bc7954c6d3f381fbae8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced <strong style=\"color:#cdcd00\">vague-glitter-57</strong>: <a href=\"https://wandb.ai/albergoni-nicolo/Tesi/runs/30p0fn2d\" target=\"_blank\">https://wandb.ai/albergoni-nicolo/Tesi/runs/30p0fn2d</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>.\\data\\wandb_logs\\wandb\\run-20220320_171858-30p0fn2d\\logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Successfully finished last run (ID:30p0fn2d). Initializing new run:<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.12.11"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\Users\\alber\\Desktop\\DeepLabV3Plus-Augmented-SuperResolution\\data\\wandb_logs\\wandb\\run-20220320_171952-47fbqzlc</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href=\"https://wandb.ai/albergoni-nicolo/Tesi/runs/47fbqzlc\" target=\"_blank\">sage-aardvark-58</a></strong> to <a href=\"https://wandb.ai/albergoni-nicolo/Tesi\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/47 [00:09<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_24060/828224115.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      7\u001B[0m )\n\u001B[0;32m      8\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 9\u001B[1;33m \u001B[0mtuner\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msearch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32mc:\\users\\alber\\desktop\\deeplabv3plus-augmented-superresolution\\venv\\lib\\site-packages\\keras_tuner\\engine\\base_tuner.py\u001B[0m in \u001B[0;36msearch\u001B[1;34m(self, *fit_args, **fit_kwargs)\u001B[0m\n\u001B[0;32m    177\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    178\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mon_trial_begin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrial\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 179\u001B[1;33m             \u001B[0mresults\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrun_trial\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrial\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0mfit_args\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mfit_kwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    180\u001B[0m             \u001B[1;31m# `results` is None indicates user updated oracle in `run_trial()`.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    181\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mresults\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_24060/1653534344.py\u001B[0m in \u001B[0;36mrun_trial\u001B[1;34m(self, trial, **kwargs)\u001B[0m\n\u001B[0;32m     30\u001B[0m         \u001B[0mwandb\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlr\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msuperres_args\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"learning_rate\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     31\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 32\u001B[1;33m         \u001B[0mmetric\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcompute_superresolution_output\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mprecomputed_data_paths\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msuperres_args\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdest_folder\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mSUPERRES_OUTPUT_DIR\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"slice\"\u001B[0m\u001B[1;33m,\u001B[0m  \u001B[0mnum_aug\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mNUM_AUG\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mglobal_normalize\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mglobal_normalize\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msave_output\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     33\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     34\u001B[0m         \u001B[0mrun\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlog\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m{\u001B[0m\u001B[1;34m\"mean_iou\"\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mmetric\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_24060/1794882917.py\u001B[0m in \u001B[0;36mcompute_superresolution_output\u001B[1;34m(precomputed_data_paths, superres_args, dest_folder, mode, num_aug, global_normalize, save_output)\u001B[0m\n\u001B[0;32m     42\u001B[0m         \u001B[0mglobal_max\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreduce_max\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mclass_masks\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mglobal_normalize\u001B[0m \u001B[1;32melse\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     43\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 44\u001B[1;33m         \u001B[0mclass_masks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmap_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfn\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mlambda\u001B[0m \u001B[0mimage\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mmin_max_normalization\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mimage\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnumpy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnew_min\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0.0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnew_max\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1.0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mglobal_min\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mglobal_min\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mglobal_max\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mglobal_max\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0melems\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mclass_masks\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     45\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     46\u001B[0m         \u001B[0mtarget_image_class\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mclass_loss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msuperresolution_obj\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcompute_output\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mclass_masks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mangles\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mshifts\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_24060/1794882917.py\u001B[0m in \u001B[0;36mcompute_superresolution_output\u001B[1;34m(precomputed_data_paths, superres_args, dest_folder, mode, num_aug, global_normalize, save_output)\u001B[0m\n\u001B[0;32m     42\u001B[0m         \u001B[0mglobal_max\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreduce_max\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mclass_masks\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mglobal_normalize\u001B[0m \u001B[1;32melse\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     43\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 44\u001B[1;33m         \u001B[0mclass_masks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmap_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfn\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mlambda\u001B[0m \u001B[0mimage\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mmin_max_normalization\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mimage\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnumpy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnew_min\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0.0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnew_max\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1.0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mglobal_min\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mglobal_min\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mglobal_max\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mglobal_max\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0melems\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mclass_masks\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     45\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     46\u001B[0m         \u001B[0mtarget_image_class\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mclass_loss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msuperresolution_obj\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcompute_output\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mclass_masks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mangles\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mshifts\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_39_64.pyx\u001B[0m in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_39_64.SafeCallWrapper.__call__\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_39_64.pyx\u001B[0m in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_39_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_39_64.pyx\u001B[0m in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_39_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_39_64.pyx\u001B[0m in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_39_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_39_64.pyx\u001B[0m in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_39_64.PyDBFrame.do_wait_suspend\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32mD:\\Development\\JetBrains\\PyCharm 2021.3.1\\plugins\\python\\helpers\\pydev\\pydevd.py\u001B[0m in \u001B[0;36mdo_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[0;32m   1145\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1146\u001B[0m         \u001B[1;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_threads_suspended_single_notification\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnotify_thread_suspended\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mthread_id\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstop_reason\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1147\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_do_wait_suspend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mthread\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mframe\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mevent\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0marg\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msuspend_type\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfrom_this_thread\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1148\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1149\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_do_wait_suspend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mthread\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mframe\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mevent\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0marg\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msuspend_type\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfrom_this_thread\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Development\\JetBrains\\PyCharm 2021.3.1\\plugins\\python\\helpers\\pydev\\pydevd.py\u001B[0m in \u001B[0;36m_do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[0;32m   1160\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1161\u001B[0m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mprocess_internal_commands\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1162\u001B[1;33m                 \u001B[0mtime\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msleep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m0.01\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1163\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1164\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcancel_async_evaluation\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mget_current_thread_id\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mthread\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mid\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mframe\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "tuner = SuperresTuner(\n",
    "    # No hypermodel or objective specified.\n",
    "    max_trials=30,\n",
    "    overwrite=True,\n",
    "    directory=DATA_DIR,\n",
    "    project_name=\"tuner_trials\",\n",
    ")\n",
    "\n",
    "tuner.search()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(tuner.get_best_hyperparameters()[0].get(\"lambda_tv\"))\n",
    "print(tuner.get_best_hyperparameters()[0].get(\"lambda_eng\"))\n",
    "print(tuner.get_best_hyperparameters()[0].get(\"global_normalize\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}